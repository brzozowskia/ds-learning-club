---
title: "linear-regression"
author: Verena Haunschmid
date: "15 April 2016"
output: html_document
urlcolor: blue
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE)
opts_chunk$set(cache = TRUE)
opts_chunk$set(tidy = TRUE)
opts_chunk$set(tidy.opts=list(width.cutoff=60))
opts_knit$set(upload.fun = function(file) imgur_upload(file, key="d56260245fa1e90"), base.url = NULL) # upload all images to imgur.com
opts_chunk$set(fig.width=5, fig.height=5, cache=TRUE)
```

This is my activity for the [Data Science Learning Club Task 07: Linear Regression](http://becomingadatascientist.com/learningclub/thread-160.html).

I mostly followed [this tutorial](http://www.statmethods.net/stats/regression.html) for the linear modeling part. With the plots and data summary (correlation & statistical tests) I came up myself.

This document contains:

* Ways to visualise datasets with numerical and categorical variables
* Ways to display and find correlations between different types of variables
* Links to more detailed explanations
* Training of different linear models
* Visualisation of trained models
* Comparison of different models

# Setup

This are the libraries for plotting and other analyses.

```{r}
library(ggplot2) # general plotting
library(GGally) # ggpairs
library(knitr) # to be able to use nice looking tables (kable()) 
library(heplots) # for eta (ANOVA)
library(reshape2) # prepare data for ggplot2
library(scales)
```

# Data

I used a dataset of salaries described [here](http://data.princeton.edu/wws509/datasets/#salary). I downloaded the txt file and since it was separated by multiple spaces I reformatted it first. For this purpose I opened it in a text editor and replace "  " (two spaces) by " " (one space) repeatedly. Since there is still a space before each row we could remove them manually or just ignore the first column after reading it into R.

```{r}
salary <- read.csv("/Volumes/Vero/Data/learning-club/regression/salary.dat.txt", sep=" ")
salary <- salary[,-1] # remove first column
```

The website of the dataset also contains a description of the variables.

* sx = Sex, coded 1 for female and 0 for male
* rk = Rank, coded
        * 1 for assistant professor,
        * 2 for associate professor, and
        * 3 for full professor
* yr = Number of years in current rank
* dg = Highest degree, coded 1 if doctorate, 0 if masters
* yd = Number of years since highest degree was earned
* sl = Academic year salary, in dollars.

## Correlation & Summary

First I want to get some information about how well my features correlate with the target `salary`. [This stackexchange answer](http://stats.stackexchange.com/a/112674/12947) gives an overview over which tests to use in different situations (w.r.t. do datatype of your features).

### Numerical data

First we want to find the correlation between our numerical features `yr` and `yd` and our target `sl`.

```{r}
kable(cor(salary[,c("yr", "yd", "sl")]), col.names=c("years rank", "years degree", "salary"))
```

This can also be done for each pair of variables by using `cor.test`. This takes many lines of code if you have many features but also provides more information.

```{r]}
cor.test(x = salary[,"yr"], y = salary[,"sl"])
cor.test(x = salary[,"yd"], y = salary[,"sl"])
```
The variable `yr` (years with this rank) are more correlated with **salary** than `yd` (years with degree).

### Categorical

To find correlations between categorical variables we can use `chisq.test` (Chi-squared test for independence) or `fisher.test` (Fisher's exact test). Because of my small sample size I chose `fisher.test`.

```{r}
sx_rk <- table(salary[,c("sx", "rk")])
sx_rk
```

```{r}
fisher.test(x=as.matrix(sx_rk))
```

We can not reject the null hypothesis, that the variables `sx` and `rk` are independent.

### Categorical & Numerical data

Here I'll look at the correlation between the two categorical variables (`sx` and `rk`) and the target `sl`.

```{r}
ggplot(salary) + geom_boxplot(aes(x=sx, y=sl))
```

```{r}
ggplot(salary) + geom_boxplot(aes(x=rk, y=sl))
```

```{r}
ggplot(salary) + geom_boxplot(aes(x=dg, y=sl))
```

[This stackexchange answer](http://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618) provides a very good overview over possibilities. The author of this answer describes the following approach where you use ANOVA to find the correlation between `sl` and the *predicted* values of `sl` based on `sx`. It is not directly possible to find a correlation since `sx` is categorical.

```{r}
model.aov <- aov(sl ~ sx, data=salary)
summary(model.aov)
etasq(model.aov, partial = FALSE)
```

```{r}
model.aov <- aov(sl ~ rk, data=salary)
summary(model.aov)
etasq(model.aov, partial = FALSE)
```

```{r}
model.aov <- aov(sl ~ dg, data=salary)
summary(model.aov)
etasq(model.aov, partial = FALSE)
```

From the plots it looks like both `sx` (sex) and `rk` (rank) are correlated with salary. The computed eta values, which can not be seen as the correlation coefficient but give a similar idea of the data, it looks like rank is more correlated with salary than sex.
Since we also not that sex and rank might not be independent, we can definitely not say that there is a causal relationship between either sex and salary or rank and salary.

`dg` (degree) does not seem to be very promising to contain information about salary.

## Visualisation

Besides summaries and correlation analysis, visualisation is an import step prior to training models. I have already shown a few plots above and this section will show a few more plots.

In a dataset with more than two features you need to make decisions on which features to plot. The y-axis should definitely depict the target value and the x-axis one of the numerical values. I chose `yr` because it's the one numerical variable that's correlated the most with salary. Categorical values can easily be included by using different colors and shapes.

```{r}
ggplot(salary, aes(x = yr, y = sl, col = rk, shape = sx)) + geom_point()
```

We can see in the above plot that most high-paying positions are held by men, but those are also the ones that are in their position for the longest time. It seems that rank is very influental on the salary (which makes lot of sense).

For comparing I also created a pairs plot between all numerical columns and used sex for coloring in one plot and rank in the second.

```{r}
ggpairs(salary, mapping=aes(col=sx, alpha=0.5), columns=which(colnames(salary) %in% c("sl", "yd", "yr")))
```

From the above plot you can see both variables show some positive correlation with salary but that this correlation is not as high for females as for males.

```{r}
ggpairs(salary, mapping=aes(col=rk, alpha=0.5), columns=which(colnames(salary) %in% c("sl", "yd", "yr")))
```
From this plot you can see that rank could be very helpful in determining the salary since the three densities don't overlap too much.

# Fit a simple model

The simplest idea is to fit a model on all available features.

```{r}
fit1 <- lm(sl ~ sx + rk + yr + dg + yd, data = salary)
```

```{r}
set.seed(65848)
train_idx <- sample(1:nrow(salary), round(nrow(salary) * 0.75))
train <- salary[train_idx,]
test <- salary[-train_idx,] # exclude all that are in train_idx
fit <- lm(sl ~ sx + yr, data=train)
```

## Check out the model

When we've trained a model, there are many functions to investigate this model. In this section I am just trying each function from the tutorial.

`summary` shows a summary of the residuals, the coefficients and some statistics. We can also see that our model actually only uses `yr` and `sxmale`.

**Side note: R performs [dummy coding](http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm#dummy) for categorical data.**

```{r}
summary(fit)
```

We can also just look at the coefficients.

```{r}
coefficients(fit) # model coefficients
```

We can also compute confidence intervals for the model parameters.

```{r}
confint(fit, level=0.95) # CIs for model parameters 
```

Sometimes it's also useful to see the prediction for all training samples.

```{r}
fitted(fit) # predicted values
```

You can also look at the [residual](https://en.wikipedia.org/wiki/Errors_and_residuals), the difference between the **target** and the **predicted** value.

```{r}
residuals(fit) # residuals
```

With ANOVA performed on one model you can test the significance of the model terms.

```{r}
anova(fit) # anova table 
```

The next method `vcov` gives you the variance-covariance matrix of the model parameters.

```{r}
vcov(fit) 
```

The tutorial mentioned above also mentions the function `influence` but it has so much output that I'll leave it out.

## Create predictions

We just used 75% of the samples for training. On the remaining 25% we can compute the test error (generalization error).

```{r}
pred_test <- predict(fit, test)  # it automatically picks the right columns
sum((pred_test - test$sl)^2) # residual sum of squares
```

For comparison we need the error on the training set.

```{r}
pred_train <- predict(fit, train) 
sum((pred_train - train$sl)^2) 
```

More interesting for a user would be the mean error the model makes in the currency of the salary and not the sum of the squared errors over all samples.

```{r}
sqrt(sum((pred_test - test$sl)^2) / length(pred_test)) # root mean squared error
```

```{r}
sqrt(sum((pred_train - train$sl)^2) / length(pred_train))
```

# Automate this process

In general you want to train different models with different features and you also want to train the same model several times on different splits of the data (e.g. cross-validation, leave-one-out-cross-validation). Therefore I wrote a method that trains a model on the given training set and computes training and test error. Also I have written a method to compute the residual sum of squares.

## Residual sum of sqaures

```{r}
get_residual_sum_of_squares <- function(prediction, target) {
  return (sum((prediction-target)^2))
}
```

## Root mean squared error

```{r}
rmsd <- function(prediction, target) {
  return(sqrt(get_residual_sum_of_squares(prediction, target)/length(prediction)))
}
```

## Build model and compute error

```{r}
build_model <- function (train, test, features, target) {
  form <- as.formula(paste(target, "~",paste(features,collapse="+"))) # this is a way to paste feature names dynamically into a formula
  fit <- lm(form, data = train)
  train_error <- rmsd(predict(fit, train), train[,target])
  test_error <- rmsd(predict(fit, test), test[,target])
  return(list("train_error"=train_error, "test_error"=test_error, "model"=fit))
}
```

## Use `build_model` to automate training

Here I manually perform **Leave One Out Cross Validation**. In the tutorial I mentioned above they show how to use library `DAAG` to automatically perform cross validation. The first model I build is using all features.

```{r}
train_order <- sample(nrow(salary))
CV <- list()
errors <- NULL
for (t in train_order) {
  CV[[length(CV) + 1]] <- build_model(salary[-t,], salary[t,], c("yd", "sx", "rk", "yr", "dg"), "sl")
  errors <- rbind(errors, data.frame("train" = CV[[length(CV)]]$train_error, "test" = mean(CV[[length(CV)]]$test_error)))
}
```

```{r}
errors_melt <- melt(errors, id.vars=NULL)
ggplot(errors_melt) + geom_boxplot(aes(x=variable, y=value))
```

The test errors are more wide spread than the trainings errors but interestingly the mean test error is below the mean training error.

```{r}
ggplot(errors) + geom_point(aes(x=train, y=test))
```

```{r}
train_err_all <- mean(errors$train)
train_err_all
test_err_all <- mean(errors$test)
test_err_all
```

A small training error is usually associated with a higher test error, which is very true for our dataset and model.

For comparison we can train a model separately for each feature to see which performs best. **Please note:** Training a model on a cateogorical value ends up assigning the mean salary value for each class.

```{r}
CV_feat <- list()
featnames <- c("sx", "rk", "yr", "dg", "yd")
errors_feat <- NULL
for (f in  featnames) {
  f_ix <- which(featnames==f)
  CV_feat[[f_ix]] <- list()
  for (t in train_order) {
    n <- length(CV_feat[[f_ix]]) + 1
    CV_feat[[f_ix]][[n]] <- build_model(salary[-t,], salary[t,], f, "sl")
    errors_feat <- rbind(errors_feat, data.frame("train"=CV_feat[[f_ix]][[n]]$train_error, "test"=CV_feat[[f_ix]][[n]]$test_error, "feat"=f))
  }
}
```

For the next plot I split the it into subfigures by using `facet_wrap`. For easier comparison I `rbind` the errors from the model with all features to the errors on the new models.
```{r}
ggplot(rbind(melt(errors_feat, id.vars = "feat"), data.frame(errors_melt, feat="all"))) + geom_boxplot(aes(x=variable, y=value)) + facet_wrap(~feat)
```

We can see that both the training and test error of the model with all features is very similar to the model with only using the `rank` variable. **Remember:** The trained model with all features did actually end up having coefficients only for `rk` and `sxmale`.

```{r}
ggplot(rbind(errors_feat, data.frame(errors, feat="all"))) + geom_point(aes(x=train, y=test)) + facet_wrap(~feat) + xlab("training error") + ylab("test error")
```

```{r}
train_err <- aggregate(train~feat, errors_feat, mean)
test_err <- aggregate(test~feat, errors_feat, mean)
kable(cbind(train_err, test_err))
```

Saving the smallest (`rk`) for later reporting.
```{r}
train_err_rk <- train_err[train_err$feat=="rk","train"]
test_err_rk <- test_err[test_err$feat=="rk","test"]
```

## Build model on most promising features

Ok, so this step is probably not the most scientific approach, but I'll try to learn another model on two features that I think (from looking at the visuals and the correlations) would work best. 

From the numerical values, `yr` looked most promising and had the highest correlation with salary. From the categorical values `rk` looked most promsing.

```{r}
CV_rkyr <- list()
errors_rkyr <- NULL
for (t in train_order) {
  CV_rkyr[[length(CV_rkyr) + 1]] <- build_model(salary[-t,], salary[t,], c("rk", "yr"), "sl")
  errors_rkyr <- rbind(errors, data.frame("train" = CV_rkyr[[length(CV_rkyr)]]$train_error, "test" = mean(CV_rkyr[[length(CV_rkyr)]]$test_error)))
}
```

```{r}
ggplot(melt(errors_rkyr, id.vars=NULL)) + geom_boxplot(aes(x=variable, y=value))
```

```{r}
train_err_rkyr <- mean(errors_rkyr$train)
train_err_rkyr
test_err_rkyr <- mean(errors_rkyr$test)
test_err_rkyr
```

## Build three most promising models

From looking at the training and test errors, I will pick the three most promising *feature combinations* and train them again on the whole dataset. This will not make much of a difference since for each model I have only left out one sample each, but it would also not make sense to randomly choose one model over the other. **More on how this should be done in a real project with more samples can be found at the end of this section.**

This will be the three models trained:
* Using only `rk` (outperformed all other features in the single-feature approach)
* Using all features
* Using `rk` and `yr`

```{r}
model_errors <- data.frame("model"=c("rk", "all", "rk_yr"), "train_error"=c(train_err_rk, train_err_all, train_err_rkyr), "test_error"=c(test_err_rk, test_err_all, test_err_rkyr))
kable(model_errors)
```
```{r}
ggplot(model_errors) + geom_point(aes(x = train_error, y=test_error, col=model))
```

From this plot it actually looks like my intuition about the best features was right.

The way how I defined `build_model` is not optimal for this because it requires a test. I would still like to have a function that creates the formula from the feature names and returns the model in one call.

```{r}
train_model <- function (train, features, target) {
  form <- as.formula(paste(target, "~",paste(features,collapse="+"))) # this is a way to paste feature names dynamically into a formula
  fit <- lm(form, data = train)
  return(fit)
}
```

```{r}
model_all <- train_model(salary, featnames, "sl")
model_rk <- train_model(salary, "rk", "sl")
model_rkyr <- train_model(salary, c("rk", "yr"), "sl")
```

```{r}
coefficients(model_all)
```
Interestingly, now the model takes into account mor then just `yr` and `sxmale`. Also `dgmaster` is included although it not seem (from the visualisations) that it had much influence.

```{r}
coefficients(model_rk)
```

```{r}
coefficients(model_rkyr)
```

```{r}
rmsd_all <- rmsd(predict(model_all, salary), salary$sl)
rmsd_rk <- rmsd(predict(model_rk, salary), salary$sl)
rmsd_rkyr <- rmsd(predict(model_rkyr, salary), salary$sl)
df <- data.frame("model"=c("all", "rk", "rkyr"), "rmsd"=c(rmsd_all, rmsd_rk, rmsd_rkyr))
kable(df)
```

From these numbers (**error on the training set**) it's obvious that the model trained only on `rk` is not a good choice. Using the model with `all` features might not be the best choice either. Although it has the best RMSD (again: on the training set), it uses many features. Since both models `model_all` and `model_rkyr` have almost the same performs, I'd suggest to pick the **second best** model `model_rkyr` which could prevent overfitting by keeping the model *simple*. 

### Attention - model selection

How I trained and chose the final model is not exactly the correct way. In general I would put aside a **holdout set** of 10-20 % of the samples. On the other set I would perform cross validation (k-fold or leave one out CV) and then retrain the most promising models on the everything **except** the holdout set. Than I would have, let's say, 3 promising models and the final decision which model to chose would be determined by performance on the completely unseen holdout set. Unfortunately there are only 52 samples in this set, so defining a holdout set would probably be just very random and could also help picking the wrong model.


# Further notes

[Here](http://docs.ggplot2.org/0.9.3.1/fortify.lm.html) a list of plots that can be performed to get more insight into a model. I did not yet have time to look into them and understand them. [This stackoverflow answer](http://stats.stackexchange.com/a/65864/12947) gives lots of insights in how to interpret such plots, currently it's still on my to-read list.